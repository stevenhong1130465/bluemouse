"""
çœŸæ­£çš„å››å±¤å¯„ç”Ÿ AI
å±¤æ¬¡ 1: å¯„ç”Ÿ Antigravity (MCP Tool)
å±¤æ¬¡ 2: å¯„ç”Ÿæœ¬åœ° AI (Ollama/LM Studio)
å±¤æ¬¡ 3: ç’°å¢ƒè®Šæ•¸ (BYOK)
å±¤æ¬¡ 4: æ‰‹å‹•å¯„ç”Ÿ
"""

import os
import json
from pathlib import Path
from typing import Dict, Any, Optional, Tuple
import asyncio

# requests æ˜¯å¯é¸ä¾è³´ (ç”¨æ–¼æœ¬åœ° AI æª¢æ¸¬)
try:
    import requests
    HAS_REQUESTS = True
except ImportError:
    HAS_REQUESTS = False
    print("âš ï¸ requests æ¨¡çµ„æœªå®‰è£,æœ¬åœ° AI æª¢æ¸¬åŠŸèƒ½å°‡è¢«ç¦ç”¨")
    print("å®‰è£: pip3 install --break-system-packages requests")


class TrueParasiteAI:
    """
    çœŸæ­£çš„å››å±¤å¯„ç”Ÿ AI
    è‡ªå‹•æª¢æ¸¬ä¸¦ä½¿ç”¨æ‰€æœ‰å¯ç”¨çš„ AI è³‡æº
    """
    
    def __init__(self):
        self.workspace = Path.cwd()
        self.api_key = None
        self.provider = None
        
        # æœ¬åœ° AI é…ç½®
        self.local_ai_endpoints = {
            'ollama': 'http://localhost:11434',
            'lmstudio': 'http://localhost:1234',
            'textgen': 'http://localhost:5000'
        }
    
    async def generate(self, prompt: str, temperature: float = 0.7) -> str:
        """
        å››å±¤å¯„ç”Ÿç­–ç•¥
        
        å±¤æ¬¡ 1: å¯„ç”Ÿ Antigravity (MCP Tool)
        å±¤æ¬¡ 2: å¯„ç”Ÿæœ¬åœ° AI (Ollama/LM Studio)
        å±¤æ¬¡ 3: ç’°å¢ƒè®Šæ•¸ (BYOK)
        å±¤æ¬¡ 4: æ‰‹å‹•å¯„ç”Ÿ
        """
        
        print("\nğŸ¦  å•Ÿå‹•å››å±¤å¯„ç”Ÿ AI...")
        print("=" * 60)
        
        # å±¤æ¬¡ 1: å¯„ç”Ÿ Antigravity
        if self._in_antigravity():
            print("  [1/4] ğŸ¯ æª¢æ¸¬åˆ° Antigravity ç’°å¢ƒ")
            try:
                result = await self._antigravity_parasite(prompt, temperature)
                print("  âœ… å¯„ç”Ÿ Antigravity æˆåŠŸ!")
                return result
            except Exception as e:
                print(f"  âš ï¸ Antigravity å¯„ç”Ÿå¤±æ•—: {e}")
        else:
            print("  [1/4] â­ï¸ æœªæª¢æ¸¬åˆ° Antigravity ç’°å¢ƒ")
        
        # å±¤æ¬¡ 2: å¯„ç”Ÿæœ¬åœ° AI
        print("  [2/4] ğŸ” æª¢æ¸¬æœ¬åœ° AI...")
        local_ai_type, local_ai_url = await self._detect_local_ai()
        if local_ai_type:
            print(f"  ğŸ¯ æª¢æ¸¬åˆ°æœ¬åœ° AI: {local_ai_type}")
            try:
                result = await self._local_ai_parasite(prompt, local_ai_type, local_ai_url)
                print(f"  âœ… å¯„ç”Ÿæœ¬åœ° AI ({local_ai_type}) æˆåŠŸ!")
                return result
            except Exception as e:
                print(f"  âš ï¸ æœ¬åœ° AI å¯„ç”Ÿå¤±æ•—: {e}")
        else:
            print("  â­ï¸ æœªæª¢æ¸¬åˆ°æœ¬åœ° AI")
        
        # å±¤æ¬¡ 3: ç’°å¢ƒè®Šæ•¸ (BYOK)
        print("  [3/4] ğŸ” æª¢æ¸¬ç’°å¢ƒè®Šæ•¸...")
        try:
            result = await self._auto_parasite(prompt, temperature)
            print("  âœ… ä½¿ç”¨ç’°å¢ƒè®Šæ•¸ API Key æˆåŠŸ!")
            return result
        except Exception as e:
            print(f"  âš ï¸ æœªæ‰¾åˆ°ç’°å¢ƒè®Šæ•¸: {e}")
        
        # å±¤æ¬¡ 4: æ‰‹å‹•å¯„ç”Ÿ
        print("  [4/4] ğŸ¯ å•Ÿå‹•æ‰‹å‹•å¯„ç”Ÿæ¨¡å¼...")
        result = await self._manual_parasite(prompt)
        print("  âœ… æ‰‹å‹•å¯„ç”ŸæˆåŠŸ!")
        return result
    
    def _in_antigravity(self) -> bool:
        """
        æª¢æ¸¬æ˜¯å¦åœ¨ Antigravity ç’°å¢ƒä¸­é‹è¡Œ
        """
        # æª¢æŸ¥ç’°å¢ƒè®Šæ•¸
        if os.getenv('ANTIGRAVITY_MODE'):
            return True
        
        # æª¢æŸ¥æ˜¯å¦æœ‰ MCP é€£æ¥
        if os.getenv('MCP_SERVER_NAME'):
            return True
        
        # æª¢æŸ¥å·¥ä½œç›®éŒ„
        if '.antigravity' in str(Path.cwd()):
            return True
        
        return False
    
    async def _antigravity_parasite(self, prompt: str, temperature: float) -> str:
        """
        å±¤æ¬¡ 1: å¯„ç”Ÿ Antigravity
        é€šé MCP Tool è®“ Antigravity çš„ AI ä»£ç‚ºç”Ÿæˆ
        
        âš ï¸ æ³¨æ„: åœ¨ Web UI ç’°å¢ƒä¸‹ä¸ä½¿ç”¨æ­¤æ¨¡å¼ï¼ˆæœƒé˜»å¡ï¼‰
        """
        # ğŸ”§ æª¢æ¸¬æ˜¯å¦åœ¨API Serverç’°å¢ƒï¼ˆé¿å…é˜»å¡Webè«‹æ±‚ï¼‰
        import inspect
        frame = inspect.currentframe()
        caller_frames = inspect.getouterframes(frame)
        
        # å¦‚æœèª¿ç”¨æ£§ä¸­æœ‰ api_server, ç›´æ¥æ‹‹å‡ºç•°å¸¸
        for frame_info in caller_frames:
            if 'api_server' in str(frame_info.filename):
                raise RuntimeError("Antigravity å¯„ç”Ÿæ¨¡å¼ä¸æ”¯æŒ Web API (æœƒé˜»å¡)ï¼Œé™ç´šåˆ°å‚™ç”¨å•é¡Œ")
        
        # åªåœ¨å‘½ä»¤è¡Œç’°å¢ƒä½¿ç”¨äº¤äº’å¼æ¨¡å¼
        print("\nâš ï¸ Antigravity å¯„ç”Ÿæ¨¡å¼éœ€è¦æ‰‹å‹•äº¤äº’")
        print("åœ¨ API Server ç’°å¢ƒä¸‹ä¸å¯ç”¨ï¼Œé™ç´šåˆ°å‚™ç”¨å•é¡Œ")
        
        raise RuntimeError("Antigravity å¯„ç”Ÿæ¨¡å¼åƒ…æ”¯æŒå‘½ä»¤è¡Œç’°å¢ƒ")
    
    async def _detect_local_ai(self) -> Tuple[Optional[str], Optional[str]]:
        """
        å±¤æ¬¡ 2: æª¢æ¸¬æœ¬åœ° AI æœå‹™
        è¿”å›: (ai_type, base_url)
        """
        # å¦‚æœæ²’æœ‰ requests,è·³éæœ¬åœ° AI æª¢æ¸¬
        if not HAS_REQUESTS:
            return None, None
        
        for ai_type, base_url in self.local_ai_endpoints.items():
            try:
                if ai_type == 'ollama':
                    response = requests.get(f'{base_url}/api/tags', timeout=1)
                    if response.status_code == 200:
                        return ai_type, base_url
                
                elif ai_type == 'lmstudio':
                    response = requests.get(f'{base_url}/v1/models', timeout=1)
                    if response.status_code == 200:
                        return ai_type, base_url
                
                elif ai_type == 'textgen':
                    response = requests.get(f'{base_url}/api/v1/model', timeout=1)
                    if response.status_code == 200:
                        return ai_type, base_url
            
            except:
                continue
        
        return None, None
    
    async def _local_ai_parasite(self, prompt: str, ai_type: str, base_url: str) -> str:
        """
        å±¤æ¬¡ 2: å¯„ç”Ÿæœ¬åœ° AI
        èª¿ç”¨æœ¬åœ°é‹è¡Œçš„ AI æ¨¡å‹
        """
        if not HAS_REQUESTS:
            raise RuntimeError("requests æ¨¡çµ„æœªå®‰è£,ç„¡æ³•ä½¿ç”¨æœ¬åœ° AI")
        
        if ai_type == 'ollama':
            # ç²å–å¯ç”¨æ¨¡å‹
            models_resp = requests.get(f'{base_url}/api/tags')
            models = models_resp.json().get('models', [])
            
            if not models:
                raise RuntimeError("Ollama æ²’æœ‰å¯ç”¨çš„æ¨¡å‹")
            
            # ä½¿ç”¨ç¬¬ä¸€å€‹æ¨¡å‹
            model_name = models[0]['name']
            print(f"    ğŸ“¦ ä½¿ç”¨æ¨¡å‹: {model_name}")
            
            # èª¿ç”¨ç”Ÿæˆ API
            response = requests.post(f'{base_url}/api/generate', json={
                'model': model_name,
                'prompt': prompt,
                'stream': False
            })
            
            return response.json()['response']
        
        elif ai_type == 'lmstudio':
            # LM Studio ä½¿ç”¨ OpenAI å…¼å®¹ API
            response = requests.post(f'{base_url}/v1/completions', json={
                'prompt': prompt,
                'max_tokens': 2000,
                'temperature': 0.7
            })
            
            return response.json()['choices'][0]['text']
        
        elif ai_type == 'textgen':
            # text-generation-webui API
            response = requests.post(f'{base_url}/api/v1/generate', json={
                'prompt': prompt,
                'max_new_tokens': 2000
            })
            
            return response.json()['results'][0]['text']
        
        raise RuntimeError(f"ä¸æ”¯æŒçš„æœ¬åœ° AI é¡å‹: {ai_type}")
    
    async def _auto_parasite(self, prompt: str, temperature: float) -> str:
        """
        å±¤æ¬¡ 3: å¾ç’°å¢ƒè®Šæ•¸æª¢æ¸¬ API Key (åˆæ³•ä¸”é€æ˜)
        """
        
        # æª¢æ¸¬ç’°å¢ƒè®Šæ•¸ä¸­çš„ API Key
        api_keys = {
            'anthropic': os.getenv('ANTHROPIC_API_KEY'),
            'openai': os.getenv('OPENAI_API_KEY'),
            'google': os.getenv('GOOGLE_API_KEY') or os.getenv('GEMINI_API_KEY')
        }
        
        # æ‰¾åˆ°ç¬¬ä¸€å€‹å¯ç”¨çš„ API Key
        for provider, api_key in api_keys.items():
            if api_key:
                print(f"    ğŸ”‘ ä½¿ç”¨ç’°å¢ƒè®Šæ•¸: {provider.upper()}_API_KEY")
                # ä¿å­˜ä»¥ä¾›å¾ŒçºŒä½¿ç”¨
                self.api_key = api_key
                self.provider = provider
                return await self._call_with_api_key(
                    provider, api_key, prompt, temperature
                )
        
        raise RuntimeError(
            "æœªæ‰¾åˆ° API Keyã€‚è«‹è¨­ç½®ç’°å¢ƒè®Šæ•¸:\n"
            "  export ANTHROPIC_API_KEY=your_key  # æˆ–\n"
            "  export OPENAI_API_KEY=your_key     # æˆ–\n"
            "  export GEMINI_API_KEY=your_key"
        )
    
    def set_api_key(self, api_key: str, provider: str = 'anthropic'):
        """
        æ‰‹å‹•è¨­ç½® API Key
        
        Args:
            api_key: API Key å­—ç¬¦ä¸²
            provider: AI æä¾›å•† ('anthropic', 'openai', 'google')
        """
        valid_providers = ['anthropic', 'openai', 'google']
        if provider not in valid_providers:
            raise ValueError(f"ç„¡æ•ˆçš„æä¾›å•†ã€‚æ”¯æŒ: {valid_providers}")
        
        self.api_key = api_key
        self.provider = provider
        print(f"âœ… å·²è¨­ç½® {provider.upper()} API Key")
    
    async def _manual_parasite(self, prompt: str) -> str:
        """
        å±¤æ¬¡ 3: æ‰‹å‹•å¯„ç”Ÿ
        å‰µå»ºæ–‡ä»¶,è®“ç”¨æˆ¶æ‰‹å‹•è§¸ç™¼ AI
        """
        
        # å‰µå»ºè«‹æ±‚æ–‡ä»¶
        request_file = self.workspace / "ğŸ¤–_AI_REQUEST.md"
        response_file = self.workspace / "ğŸ¤–_AI_RESPONSE.txt"
        
        # å¯«å…¥è«‹æ±‚
        with open(request_file, 'w', encoding='utf-8') as f:
            f.write(f"""# ğŸ¤– è—åœ–å°è€é¼ éœ€è¦ AI å¹«åŠ©

{prompt}

---

## ğŸ“ è«‹å¹«å¿™:

1. **é¸ä¸­ä¸Šé¢çš„å…§å®¹** (å¾ "åˆ†æéœ€æ±‚" é–‹å§‹)
2. **æŒ‰ Cmd+K** (æˆ–é»æ“Š AI æŒ‰éˆ•)
3. **è®“ AI è™•ç†**
4. **è¤‡è£½ AI çš„å›ç­”**
5. **è²¼åˆ°** `ğŸ¤–_AI_RESPONSE.txt` æ–‡ä»¶ä¸­
6. **ä¿å­˜**
7. **å›åˆ°çµ‚ç«¯æŒ‰ Enter**

---

**åªéœ€è¦ 5 ç§’!** âš¡

**æ„Ÿè¬!** ğŸ­
""")
        
        # è‡ªå‹•æ‰“é–‹æ–‡ä»¶
        try:
            os.system(f"open '{request_file}'")
        except:
            pass
        
        # æç¤ºç”¨æˆ¶
        print("\n" + "="*70)
        print("â¸ï¸  è«‹åœ¨ Antigravity ä¸­è™•ç† AI è«‹æ±‚")
        print("="*70)
        print(f"ğŸ“ æ–‡ä»¶: {request_file}")
        print("ğŸ“ æ­¥é©Ÿ:")
        print("   1. é¸ä¸­å…§å®¹")
        print("   2. æŒ‰ Cmd+K")
        print("   3. è¤‡è£½ AI å›ç­”åˆ° ğŸ¤–_AI_RESPONSE.txt")
        print("   4. ä¿å­˜ä¸¦æŒ‰ Enter")
        print("="*70)
        
        # ç­‰å¾…ç”¨æˆ¶è™•ç†
        input("\næŒ‰ Enter ç¹¼çºŒ (ç•¶ AI å®Œæˆå¾Œ)...")
        
        # è®€å–éŸ¿æ‡‰
        if response_file.exists():
            with open(response_file, 'r', encoding='utf-8') as f:
                response = f.read().strip()
            
            if response:
                # æ¸…ç†æ–‡ä»¶
                try:
                    request_file.unlink()
                    response_file.unlink()
                except:
                    pass
                
                return response
        
        raise RuntimeError("æœªæ‰¾åˆ° AI éŸ¿æ‡‰,è«‹ç¢ºä¿å·²ä¿å­˜åˆ° ğŸ¤–_AI_RESPONSE.txt")
    
    async def _call_with_api_key(
        self,
        provider: str,
        api_key: str,
        prompt: str,
        temperature: float
    ) -> str:
        """ä½¿ç”¨ API Key èª¿ç”¨ AI"""
        
        if provider in ['anthropic', 'claude']:
            return await self._call_anthropic(api_key, prompt, temperature)
        elif provider in ['openai', 'gpt']:
            return await self._call_openai(api_key, prompt, temperature)
        elif provider in ['google', 'gemini']:
            return await self._call_google(api_key, prompt, temperature)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æä¾›å•†: {provider}")
    
    async def _call_anthropic(self, api_key: str, prompt: str, temperature: float) -> str:
        """èª¿ç”¨ Anthropic Claude API"""
        try:
            import anthropic
            
            client = anthropic.Anthropic(api_key=api_key)
            
            message = client.messages.create(
                model="claude-sonnet-4",
                max_tokens=4096,
                temperature=temperature,
                messages=[{"role": "user", "content": prompt}]
            )
            
            return message.content[0].text
        
        except ImportError:
            raise RuntimeError("è«‹å®‰è£ anthropic: pip install anthropic")
    
    async def _call_openai(self, api_key: str, prompt: str, temperature: float) -> str:
        """èª¿ç”¨ OpenAI API"""
        try:
            import openai
            
            client = openai.OpenAI(api_key=api_key)
            
            response = client.chat.completions.create(
                model="gpt-4",
                temperature=temperature,
                messages=[{"role": "user", "content": prompt}]
            )
            
            return response.choices[0].message.content
        
        except ImportError:
            raise RuntimeError("è«‹å®‰è£ openai: pip install openai")
    
    async def _call_google(self, api_key: str, prompt: str, temperature: float) -> str:
        """èª¿ç”¨ Google Gemini API"""
        try:
            import google.generativeai as genai
            
            genai.configure(api_key=api_key)
            model = genai.GenerativeModel('gemini-pro')
            
            response = model.generate_content(
                prompt,
                generation_config={'temperature': temperature}
            )
            
            return response.text
        
        except ImportError:
            raise RuntimeError("è«‹å®‰è£ google-generativeai: pip install google-generativeai")


# å…¨å±€å¯¦ä¾‹
_parasite_ai = None

def get_parasite_ai() -> TrueParasiteAI:
    """ç²å–å…¨å±€å››å±¤å¯„ç”Ÿ AI å¯¦ä¾‹"""
    global _parasite_ai
    if _parasite_ai is None:
        _parasite_ai = TrueParasiteAI()
    return _parasite_ai


async def ai_generate(prompt: str, temperature: float = 0.7, api_key: str = None, provider: str = None) -> str:
    """
    ä½¿ç”¨å››å±¤å¯„ç”Ÿ AI ç”Ÿæˆå…§å®¹
    
    å››å±¤å¯„ç”Ÿç­–ç•¥:
    1. Antigravity MCP Tool (æœ€å„ªé›…)
    2. æœ¬åœ° AI (Ollama/LM Studio) (é›¶æˆæœ¬)
    3. ç’°å¢ƒè®Šæ•¸ BYOK (é€æ˜)
    4. æ‰‹å‹•å¯„ç”Ÿ (é™ç´š)
    
    Args:
        prompt: AI æç¤ºè©
        temperature: æº«åº¦åƒæ•¸ (0.0-1.0)
        api_key: å¯é¸çš„ API Key
        provider: å¯é¸çš„æä¾›å•†
    
    Returns:
        AI ç”Ÿæˆçš„å…§å®¹
    """
    parasite = get_parasite_ai()
    
    # å¦‚æœæä¾›äº† API Key,å…ˆè¨­ç½®
    if api_key and provider:
        parasite.set_api_key(api_key, provider)
    
    return await parasite.generate(prompt, temperature)


# æ¸¬è©¦å‡½æ•¸
async def test_parasite():
    """æ¸¬è©¦å¯„ç”Ÿ AI"""
    
    print("ğŸ§ª æ¸¬è©¦å¯„ç”Ÿ AI\n")
    
    prompt = """åˆ†æéœ€æ±‚: æˆ‘è¦åšä¸€å€‹é›»å•† app

è«‹ç”¨ JSON æ ¼å¼è¿”å›:
{
  "type": "ç³»çµ±é¡å‹",
  "core_features": ["åŠŸèƒ½1", "åŠŸèƒ½2"],
  "modules": ["æ¨¡çµ„1", "æ¨¡çµ„2"],
  "complexity": "ç°¡å–®/ä¸­ç­‰/è¤‡é›œ"
}

åªè¿”å› JSON,ä¸è¦å…¶ä»–æ–‡å­—ã€‚
"""
    
    try:
        response = await ai_generate(prompt)
        print(f"\nâœ… æˆåŠŸ!\n")
        print("éŸ¿æ‡‰:")
        print(response)
    
    except Exception as e:
        print(f"\nâŒ å¤±æ•—: {e}")


if __name__ == "__main__":
    asyncio.run(test_parasite())
