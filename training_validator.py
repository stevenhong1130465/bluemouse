#!/usr/bin/env python3
"""
æ‰¹é‡é©—è­‰å·¥å…· - Training Validator
è‡ªå‹•é©—è­‰æ‰€æœ‰ Python å‡½æ•¸æª”æ¡ˆ,ç”Ÿæˆé©—è­‰å ±å‘Š
"""

import os
import json
import ast
from pathlib import Path
from collections import defaultdict

def load_spec():
    """è¼‰å…¥ MMLA è¦æ ¼"""
    spec_file = Path("mmla_spec.json")
    if not spec_file.exists():
        return {}
    with open(spec_file, "r", encoding="utf-8") as f:
        return json.load(f)

def simple_validate(code: str, expected_name: str) -> dict:
    """ç°¡åŒ–çš„é©—è­‰é‚è¼¯"""
    errors = []
    
    # æª¢æŸ¥èªæ³•
    try:
        tree = ast.parse(code)
    except SyntaxError as e:
        return {"success": False, "errors": [f"èªæ³•éŒ¯èª¤: {str(e)}"]}
    
    # æª¢æŸ¥å‡½æ•¸å
    func_found = False
    for node in ast.walk(tree):
        if isinstance(node, ast.FunctionDef):
            if node.name == expected_name:
                func_found = True
                # æª¢æŸ¥é¡å‹æç¤º
                if not all(arg.annotation for arg in node.args.args):
                    errors.append("ç¼ºå°‘é¡å‹æç¤º")
                if not node.returns:
                    errors.append("ç¼ºå°‘è¿”å›é¡å‹æç¤º")
            break
    
    if not func_found:
        errors.append(f"æ‰¾ä¸åˆ°å‡½æ•¸ {expected_name}")
    
    return {
        "success": len(errors) == 0,
        "errors": errors
    }

class TrainingValidator:
    def __init__(self, project_dir: str):
        self.project_dir = Path(project_dir)
        self.results = {
            "total": 0,
            "passed": 0,
            "failed": 0,
            "errors_by_type": defaultdict(int),
            "failed_files": []
        }
    
    def find_python_files(self):
        """æ‰¾åˆ°æ‰€æœ‰ Python å‡½æ•¸æª”æ¡ˆ"""
        python_files = []
        
        # æ’é™¤æ¸¬è©¦æª”æ¡ˆå’Œå·¥å…·æª”æ¡ˆ
        exclude_patterns = [
            "test_", "setup_", "server.py", "mmla_parser.py",
            "training_validator.py", "error_generator.py"
        ]
        
        for file in self.project_dir.glob("*.py"):
            if not any(pattern in file.name for pattern in exclude_patterns):
                python_files.append(file)
        
        return python_files
    
    def extract_function_name(self, code: str) -> str:
        """å¾ä»£ç¢¼ä¸­æå–å‡½æ•¸å"""
        import ast
        try:
            tree = ast.parse(code)
            for node in ast.walk(tree):
                if isinstance(node, ast.FunctionDef):
                    return node.name
        except:
            pass
        return None
    
    def find_matching_node(self, function_name: str):
        """åœ¨ MMLA spec ä¸­æ‰¾åˆ°åŒ¹é…çš„ç¯€é» (ç°¡åŒ–ç‰ˆ)"""
        # ç°¡åŒ–ç‰ˆ:ç›´æ¥è¿”å›å‡½æ•¸åä½œç‚º ID
        return function_name
    
    def validate_file(self, file_path: Path):
        """é©—è­‰å–®å€‹æª”æ¡ˆ"""
        print(f"ğŸ“ é©—è­‰: {file_path.name}")
        
        with open(file_path, "r", encoding="utf-8") as f:
            code = f.read()
        
        # æå–å‡½æ•¸å
        function_name = self.extract_function_name(code)
        if not function_name:
            print(f"  âš ï¸  ç„¡æ³•æå–å‡½æ•¸å")
            return
        
        # æ‰¾åˆ°å°æ‡‰çš„ MMLA ç¯€é»
        node_id = self.find_matching_node(function_name)
        if not node_id:
            print(f"  âš ï¸  æ‰¾ä¸åˆ°å°æ‡‰çš„ MMLA ç¯€é»: {function_name}")
            return
        
        # åŸ·è¡Œé©—è­‰
        result = simple_validate(code, function_name)
        
        self.results["total"] += 1
        
        if result.get("success"):
            self.results["passed"] += 1
            print(f"  âœ… é€šé")
        else:
            self.results["failed"] += 1
            errors = result.get("errors", [])
            print(f"  âŒ å¤±æ•—: {len(errors)} å€‹éŒ¯èª¤")
            
            for error in errors:
                print(f"     - {error}")
                # çµ±è¨ˆéŒ¯èª¤é¡å‹
                if "signature mismatch" in error.lower():
                    self.results["errors_by_type"]["å‡½æ•¸ç°½åä¸ç¬¦"] += 1
                elif "dependencies" in error.lower():
                    self.results["errors_by_type"]["ä¾è³´æœªè²æ˜"] += 1
                elif "syntax" in error.lower():
                    self.results["errors_by_type"]["èªæ³•éŒ¯èª¤"] += 1
                elif "arguments" in error.lower():
                    self.results["errors_by_type"]["åƒæ•¸éŒ¯èª¤"] += 1
                else:
                    self.results["errors_by_type"]["å…¶ä»–éŒ¯èª¤"] += 1
            
            self.results["failed_files"].append({
                "file": file_path.name,
                "function": function_name,
                "node_id": node_id,
                "errors": errors
            })
    
    def run(self):
        """åŸ·è¡Œæ‰¹é‡é©—è­‰"""
        print("ğŸš€ é–‹å§‹æ‰¹é‡é©—è­‰...\n")
        
        python_files = self.find_python_files()
        print(f"æ‰¾åˆ° {len(python_files)} å€‹ Python æª”æ¡ˆ\n")
        
        for file in python_files:
            self.validate_file(file)
            print()
        
        self.generate_report()
    
    def generate_report(self):
        """ç”Ÿæˆé©—è­‰å ±å‘Š"""
        print("=" * 60)
        print("ğŸ“Š é©—è­‰å ±å‘Š")
        print("=" * 60)
        print(f"\nç¸½å‡½æ•¸æ•¸: {self.results['total']}")
        print(f"âœ… é€šé: {self.results['passed']} ({self.results['passed']/max(self.results['total'],1)*100:.1f}%)")
        print(f"âŒ å¤±æ•—: {self.results['failed']} ({self.results['failed']/max(self.results['total'],1)*100:.1f}%)")
        
        if self.results["errors_by_type"]:
            print("\nğŸ› éŒ¯èª¤é¡å‹çµ±è¨ˆ:")
            for error_type, count in sorted(self.results["errors_by_type"].items(), key=lambda x: x[1], reverse=True):
                print(f"  - {error_type}: {count} æ¬¡")
        
        # å„²å­˜è©³ç´°å ±å‘Š
        report_path = self.project_dir / "validation_report.json"
        with open(report_path, "w", encoding="utf-8") as f:
            json.dump(self.results, f, indent=2, ensure_ascii=False)
        
        print(f"\nğŸ“„ è©³ç´°å ±å‘Šå·²å„²å­˜: {report_path}")
        
        # ç”Ÿæˆ Markdown å ±å‘Š
        self.generate_markdown_report()
    
    def generate_markdown_report(self):
        """ç”Ÿæˆ Markdown æ ¼å¼å ±å‘Š"""
        report_path = self.project_dir / "validation_report.md"
        
        with open(report_path, "w", encoding="utf-8") as f:
            f.write("# ğŸ¯ MMLA è¨“ç·´é©—è­‰å ±å‘Š\n\n")
            f.write(f"**é©—è­‰æ™‚é–“**: {Path(__file__).stat().st_mtime}\n\n")
            
            f.write("## ğŸ“Š ç¸½é«”çµ±è¨ˆ\n\n")
            f.write(f"- ç¸½å‡½æ•¸æ•¸: {self.results['total']}\n")
            f.write(f"- âœ… é€šé: {self.results['passed']} ({self.results['passed']/max(self.results['total'],1)*100:.1f}%)\n")
            f.write(f"- âŒ å¤±æ•—: {self.results['failed']} ({self.results['failed']/max(self.results['total'],1)*100:.1f}%)\n\n")
            
            if self.results["errors_by_type"]:
                f.write("## ğŸ› éŒ¯èª¤é¡å‹åˆ†æ\n\n")
                total_errors = sum(self.results["errors_by_type"].values())
                for error_type, count in sorted(self.results["errors_by_type"].items(), key=lambda x: x[1], reverse=True):
                    percentage = count / total_errors * 100
                    f.write(f"{count}. **{error_type}**: {count} æ¬¡ ({percentage:.1f}%)\n")
                f.write("\n")
            
            if self.results["failed_files"]:
                f.write("## âŒ å¤±æ•—æª”æ¡ˆæ¸…å–®\n\n")
                for item in self.results["failed_files"]:
                    f.write(f"### {item['file']}\n")
                    f.write(f"- å‡½æ•¸: `{item['function']}`\n")
                    f.write(f"- ç¯€é» ID: `{item['node_id']}`\n")
                    f.write(f"- éŒ¯èª¤:\n")
                    for error in item['errors']:
                        f.write(f"  - {error}\n")
                    f.write("\n")
        
        print(f"ğŸ“„ Markdown å ±å‘Šå·²å„²å­˜: {report_path}")


if __name__ == "__main__":
    # ä½¿ç”¨ç•¶å‰ç›®éŒ„
    validator = TrainingValidator(".")
    validator.run()
